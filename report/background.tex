\chapter{Background}\label{C:bg}

\section{Game Theory}\label{sec:GTintro}

The concept of Game Theory exists to explain the interactions between rational actors in a mathematical way. Many situations in life involve interactions between people making optimal choices based on preferences derived from knowledge. These situations can be explained as games played by 'actors'. Actors are given a set of actions and a set of preferences over those actions. The interactions therein are known as a 'game'. There are few requirements for a situation to be explained as a game - actors must each choose one action from the total set of actions at a time, and their preferences must be consistent. \cite{fudenberg1991game, osborne2004introduction, owen2013game} As such, game theory is a vast topic encompassing a limitless number of scenarios, and is utilised by a wide variety of fields. These fields include economics, biology, politics, philosophy, and computer science.

To discuss a game's makeup more formally, we can assign some terminology. In a single-actor game, we could show the preference for an action $a$ from the set of actions $A$ as $u(a)$. If an actor prefers action $a$ over action $b$, we would say $u(a) > u(b)$. The preferences of actors are often given a value. While in standard games these values determine the strength of the preference, in ordinal games the actual values of these preferences do not matter; so long as the order of preference holds. We could, for instance, assign $u(a) = 1$ and $u(b) = 0$, or we could assign $u(a) = 100$ and $u(b) = 5$. These would be equivalent. \cite{osborne2004introduction}

In a two-actor game, a single action no longer holds a direct preference. It is the pair of actions $(a,b)$, $a,b \in A$ that make up a 'state' $s = (a,b)$ in a game - and we would likewise assign preferences for an actor $x$ - from the set of actors $P$ - to each state $u_x(s) = u_x(a,b)$. The development of preferences over these states is called learning a strategy. If these preferences and the resulting strategy of both actors are the same for their respective actions, the game is called symmetric. Once a strategy has been found, we have identified preferences over each state, and our actors can play the game.

'Playing' a game can take several forms. Sequential games offer actors turns that run until a set end-state has been reached, where typically 'winners' and 'losers' are declared. Simultaneous games deal rewards to actors after each has made an action simultaneously. Within Simultaneous games, there are two subsets: repeated and one-shot games. Repeated games recur indefinitely to test long-term strategies without clear 'winning' conditions. One-shot games, the likes of which we will be studying here, are those in which a single set of actions is made without communication and each actor tries to maximise their score for this single round. This single action is now the actor's strategy.

\section{One-Shot Games and Nash Equilibria}\label{sec:1shot}

While various forms of continuous games allow for clever, cooperative strategies to develop over time - in fact, often the longer games run the more cooperation seems to develop \cite{djiguemde2022continuous} - One-shot games almost exclusively lead to greedy strategies where each actor selfishly tries to maximise their score, sometimes to the detriment of both actors. An example of this is the Prisoner's Dilemma (PD). This is a one-shot game in which two actors - taking the role of prisoners - are talked to independently after being caught for a crime. If one of them is to pin the blame for the crime onto the other, he or she is let go and their counterpart serves the full sentence. However, if \textit{both} attempt to pin the crime on the other, they have the full sentence divided between them. If neither defers any blame, they serve a reduced sentence. We can define this as an ordinal game theory problem where actor $x$ can either cooperate $c$ with or defect $d$ from actor $y$ in which the order of preference for the states is as follows:

\[u_x(c_x,d_y) < u_x(d_x,d_y) < u_x(c_x,c_y) < u_x(d_x,c_y)\]

Looking at this with no knowledge of what $y$ will choose, we can observe that the worst result is obtained from $x$ cooperating, and the best result is obtained from $x$ defecting. The strategy becomes clear that one should always defect. Even if we know what $y$ picks, we will always maximise our utility by defecting. Of course, both actors have this knowledge, and so both choose to defect, netting the second-worst score possible for each. How do we solve this?

To unpack this problem, we must discuss this type of outcome more formally. An outcome in which neither actor can improve their score by unilaterally changing the state of the game - much like the $d_x,d_y$ state of PD, where if either chooses to cooperate, their punishment grows - is known as a Nash Equilibrium. This kind of strategy, while always considered a solution to a one-shot game, is not necessarily optimal. Indeed the state in which both actors in the PD cooperate $c_x,c_y$ provides a higher reward for both participants than $d_x,d_y$ - though this is, itself, not a Nash Equilibrium, as either actor changing their strategy to defect from $c_x,c_y$ yields a better reward.

Another game of interest in regards to Nash Equilibria is the Stag Hunt (SH). This is a one-shot game of two actors who wish to go hunting. If they work together, they can successfully hunt a stag, each taking home a large quantity of food. They are alternatively able to hunt hares which provide a lesser amount of food than their share of the stag. However, to catch a hare, one needn't work with the other hunter. If one chooses to hunt the stag, and the other hunts a hare, the stag hunter comes home with nothing. Here we get a different order of preferences than PD for actor $x$ in an ordinal scenario:

\[u_x(c_x,d_y) < u_x(d_x,d_y) \geq u_x(d_x,c_y) < u_x(c_x,c_y)\]

In this scenario, we have \textit{two} Nash Equilibria. If both actors defect (hunt hares), then one choosing to cooperate instead (hunt the stag) will result in a loss of reward. Similarly, if both are cooperating, then the change of strategy to defect will net that actor a lesser reward. This pair of equilibria is, however, unequal. Both defecting nets a worse reward for each than both cooperating. So while both are 'solutions' to the problem; one is optimal and one is not. Now the question follows; how do we encourage actors stuck in the lesser equilibrium to switch to the optimal?

These equilibria are common phenomena throughout game theory, and while continuous games allow actors to develop long-term strategies such as 'tit-for-tat' to escape from Nash Equilibria, such an equivalent does not exist for one-shot games. The primary limitation appears to be a lack of communication. In continuous games, though direct communication may not be possible, it \textit{is} possible to glean information about one's opponent through their choice of actions as the game continues. This basic form of communication through signalling actions could theoretically, if applied to one-shot games in the form of a 'pregame' display, provide a pathway out of a non-optimal Nash Equilibrium.

This pregame must have an endpoint - we need to get to the one-shot game itself at some point. The pregame therefore cannot be continuous. Equally, we do not want to force a fixed end-point after a given number of iterations, as the actors may not have decided upon a valid strategy in the time allotted. Rather, we should give the actors themselves the power to end the pregame as and when they choose. At its most basic, these pregames will take the form of an action set consisting of all the strategies for an actor within the one-shot game, the ability to not take an action, and the ability to 'leave' the pregame, initiating the one-shot with whatever strategies the actors are employing at the time of inception. The other possibility is the inclusion of an affordance to the actors in some form or another to influence the actions of their opponent more directly.

So, to conclude, we have devised a means for actors in a one-shot game to communicate in a sequential manner through the form of a pregame, where they publicly decide on the strategy they wish to employ within the actual game before it takes place. But how to enact such a means?

\section{Value Iteration}\label{sec:VIintro}

Value iterators are machine learning tools that iteratively learn a policy $\pi$ to navigate a Markov Decision Process \cite{Bel}. A Markov Decision Process (MDP) is a system of states $S$ and actions $A$ which allow the transition from a given state $s$ to a new state $s'$ via action $a$ with probability $T(s, a, s')$, supplying a reward upon arrival of $R(s, a, s')$.\cite{kochenderfer2022algorithms} These four elements; S; A; T; and R make up a representation of an environment and the relationships between states in that environment.

Our pregame is already a system of actions. A state is, as discussed before, simply a set of the different actions chosen by our actors at a given moment. We can take $T$ as a binary system where performing an action has a 100\% chance of altering the state to represent the performed act, and a 0\% chance of reaching any other state. Our reward matrix $R$ is simply 0 unless we perform the 'leave' action, in which actors are given the reward of the result of the one-shot game.

With an MDP, we develop $\pi$ by calculating a numeric value for each action from each state $Q$. This is done via the Bellman Equation as follows:

\[Q\left(s,a\right)=\sum_{s' \in S}^{ }T\left(s,a,s'\right)\cdot \left(R\left(s,a,s'\right)+\gamma\cdot V\left(s'\right)\right)\]

Where $\gamma$ is a discount factor $\leq1$ to prioritise shorter-term rewards over longer-term ones - to encourage our actors to leave the pregame in a timely manner - and where $V$ is the calculated value of each state - $0$ to start with. This equation values an action from a state as the sum of all the discounted values of all the other states and the reward of ending up in each state from here scaled by the probability that we end up in that state in the first place. From each $Q(s,a)$ value, we can determine the value of each of the states $V(s)$.

\[V\left(s\right)=\max_{a}\left(Q\left(s,a\right)\right)\]

The value of a state $s$ is simply the value of performing the action valued highest from $s$. If this process is repeated, performing $Q$ steps and $V$ steps in sequence, the value iterator will converge on optimal values for each action at each state $Q*(s,a)$. These values make our policy $\pi$, where we aim to choose the best action at each state, maximising our rewards.

\section{Related Work}

Other papers have looked into game theory solutions through AI in the past. However, none apply the concept of a pregame encounter to determine trust within an eventual one-shot game. Perhaps the closest example to this concept is found in the paper 'Learning in One-Shot Strategic Form Games' by Altman et al. \cite{altman2006learning}. This paper encompasses a machine-learning approach to learning the strategies of opponents before a one-shot game. This is done not through communication between participants as we investigate but through prior history in other one-shot games. This is not to be confused with continuous games, in which the same game is repeated between the same pair of actors ad-nauseam; this paper explores the possibility of actors playing strategies in completely separate games with different opponents revealing insights into how an actor may approach an unseen game, thus allowing actors to choose a strategy to best maximise their score.

Experiments for Altman et al.'s paper were done through university students supplied with a set of possible strategies for a set of varying one-shot games. They were instructed to pick a strategy for each game, and these strategies would be faced against the strategies chosen by other students for the chance at a prize. Of course, the competition was not the focus of the study, but rather to supply the researchers with a dataset of strategies for games with which to train a set of association rules to predict strategies for unseen games.

They were able to predict using these rules with greater than 50\% accuracy, the chosen strategy, in almost all played games. Furthermore, they determined these rules were marginally (~3\%) more successful than simply predicting the most common strategy of that player in the past. The paper concludes that though simple association rules are only marginally better than naive predictions, there is potential for more sophisticated Machine Learning methods to better learn associations and allow actors to better maximise their scores against unseen opponents in a new one-shot game.

The most significant inspiration for this work is the paper 'Holds enable one-shot reciprocal exchange' by Frean and Marsland \cite{frean2022holds}. This paper, though not directly discussing game theory, uses a value iterator to explore the origins of trading in societies through a game. They state that during an exchange, humans assume a certain risk of their trade partner simply stealing their trade item(s) and keeping everything for themselves. They therefore posit that for such a high-risk event to take such a central role in society, there must be an affordance that removes or reduces such risk.

A game is established in which two actors have possession of an item. They each value the other's item more than their own, though they still value their own more than having nothing. The actors wish to maximise the value in their possession and can leave the encounter whenever they wish. Ideally, an actor would leave with both items, resulting in the largest score. In this basic form, they established a value iterator to learn the best policy to navigate the exchange, and both actors simply left with their own items.

Clearly, the actors determine that attempting to swap items is too much risk. So how to overcome this? By the advent of affordances - specifically: holds. A hold is the ability to 'take hold of' the other actor, restricting the ability of both to leave the encounter until the 'holder' decides to release their hold. Running the experiment again yielded a successful exchange! The actor with the first turn will take hold of the other, one by one they will release their items, and then take possession of the other's item before the first actor releases their hold and the pair exits.

Another form of holds deemed successful is the ability for both actors to hold the same item. Neither can leave if both hold the same item and so the same basic structure emerges. As for why neither actor attempts to take both items for themselves, the pair cleverly structure the encounter around never leaving oneself open for having both items held by their opponent without an exit-limiting hold in place.

Finally, Frean and Marsland determine a form of holds deemed implicit holds, which act as a way for each actor to believe they will be restrained from leaving without the other's consent without any physical holds taking place. In this scenario, the pair will happily release their items and perform a successful swap before leaving. This, it was determined, is the system with which our current daily exchanges take place. The belief that one will be stopped from abusing the system allows a pair of traders to exchange goods without fear of such abuse.

The implications for the seeming ease of this process raise questions as to why implicit trading appears to be a human-only behaviour, though the fact that such a solution exists in the first place provides a thorough insight into how the foundations of our society were established.

An interesting note from this paper is the apparent relation between the item swappers and the Prisoner's Dilemma. If we observe the order of potential exit-state reward preferences, this should reveal itself. For actor $x$ against actor $y$ where $c$ is having released one's own item in favour of the other, and $d$ is retaining hold of one's own item, we obtain states; $c_x,c_y$ - a successful swap; $d_x,c_y$ - $x$ leaving with both items; $c_x,d_y$ - $y$ leaving with both items; and $d_x,d_y$ - no swap taking place. The order of preference for $x$ here is as follows:

\[u_x(c_x,d_y) < u_x(d_x,d_y) < u_x(c_x,c_y) < u_x(d_x,c_y)\]

This is the same as in PD. This interesting relationship may be worth exploring.
