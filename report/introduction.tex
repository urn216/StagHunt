\chapter{Introduction}\label{C:intro}

At the heart of many activities in our lives are one-off decisions. "Should I go for a walk?", "Should I have a slice of cake?". These decisions are typically made through careful consideration of the benefits and consequences of these actions - eating cake is an enjoyable experience but may lead to health complications down the road. Some decisions, however, are made involving one or more other people. Often these people are friends - but sometimes these people are strangers, and on some occasions, we aren't able to even see the other person or people involved - if we're deciding whether or not to go to war, for instance. 

The extent to which this lack of understanding of another person affects one's decision-making is a topic typically explored through the guise of philosophy. A topic unexplored as of yet is whether we can show or even quantify the importance of knowledge of others for decision-making within a machine learning model. In this paper, we study this idea, testing the limits of trust and observing the change in the quality of decisions made between strangers and between people we know and maybe even have some influence over.

Understanding the motives of those involved in a decision allows for better choices through interactions. The question thus boils down to: just what problems does a lack of understanding give rise to, and how do we overcome these issues in an interaction between strangers? The answer ultimately lies within game theory. We posit here that representing a series of possible decisions as one-shot 'games', occurring once between strangers with no prior interactions, will divulge an answer. The aim of this study is to investigate these one-shot games, discover by what means we can allow knowledge of opponent behaviours to emerge, and ultimately generate rational decision-making actors who can play a series of games that mimic these real-life interactions. 

On their own, these one-shot games provide neither player with any context with which to determine how the other player(s) will act. However, with the use of a value iterator to learn an arbitrary-length pregame, we are able to build trust between actors. Primarily through the analysis of the Prisoner's Dilemma and the Stag Hunt, without telling the actors directly to trust one another, and instead motivating them purely through selfish means, we demonstrate that by the inclusion of a short pregame signalling act between the actors, they can gather enough information to mitigate the majority of the risk in their decision-making process - often leading to the emergence of cooperation where it would otherwise be dangerous to consider. 
